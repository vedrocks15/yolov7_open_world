{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9d203e-0adb-4018-a572-a82f2148c941",
   "metadata": {},
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b012d34-93b9-4ad7-b5f4-de4532d9b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File handling imports\n",
    "import pickle\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "# Modelling imports\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.utils.data\n",
    "import yaml\n",
    "from torch.cuda import amp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helpers....\n",
    "import test \n",
    "from models.experimental import attempt_load\n",
    "\n",
    "# calling a new updated code for CLIP training\n",
    "from models.yolo_zsd import Model\n",
    "\n",
    "from utils.autoanchor import check_anchors\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \\\n",
    "    fitness, strip_optimizer, get_latest_run, check_dataset, check_file, check_git_status, check_img_size, \\\n",
    "    check_requirements, print_mutation, set_logging, one_cycle, colorstr\n",
    "from utils.google_utils import attempt_download\n",
    "\n",
    "# Loss function...\n",
    "from utils.loss import ComputeLoss, ComputeLossOTA, ComputeLossOTA_CLIP, ComputeLoss_CLIP\n",
    "from utils.plots import plot_images, plot_labels, plot_results, plot_evolution\n",
    "from utils.torch_utils import ModelEMA, select_device, intersect_dicts, torch_distributed_zero_first, is_parallel\n",
    "from utils.wandb_logging.wandb_utils import WandbLogger, check_wandb_resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30c474-0878-4b77-a943-7d6f12d3a0e5",
   "metadata": {},
   "source": [
    "# Defining HyperParams for open world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a79a9c-2cc9-4c90-b86c-f62b1f67d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up cmd line arguments....\n",
    "class agP():\n",
    "    def __init__(self):\n",
    "        self.weights = \"./best_supervised_65.pt\"\n",
    "        self.cfg  = \"/Users/vedant_j/Desktop/open_world/yolov7_open_world/cfg/training/yolo_v7_open_world.yaml\"\n",
    "        self.data = \"/Users/vedant_j/Desktop/open_world/yolov7_open_world/data/open_world_65_split_yolo.yaml\"\n",
    "        self.hyp  = \"/Users/vedant_j/Desktop/open_world/yolov7_open_world/data/hyp.scratch.p5.open_world.yaml\"\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 2\n",
    "        self.img_size = [224, 224]\n",
    "        self.rect = False\n",
    "        self.resume = False\n",
    "        self.nosave = False\n",
    "        self.notest = False\n",
    "        self.noautoanchor = False\n",
    "        self.evolve = False\n",
    "        self.cache_images = False\n",
    "        self.image_weights = False\n",
    "        self.device = \"cpu\"\n",
    "        self.multi_scale = False\n",
    "        self.single_cls = False\n",
    "        self.adam = False\n",
    "        self.sync_bn = False\n",
    "        self.local_rank = -1\n",
    "        self.workers = 8\n",
    "        self.entity = None\n",
    "        self.exist_ok = False\n",
    "        self.quad = False\n",
    "        self.linear_lr = False \n",
    "        self.label_smoothing = 0.0\n",
    "        self.upload_dataset = False\n",
    "        self.bbox_interval = -1\n",
    "        self.save_period = -1\n",
    "        self.artifact_alias = \"latest\"\n",
    "        self.freeze = [0]\n",
    "        self.v5_metric = False\n",
    "        self.open_world = True\n",
    "        self.open_world_emb_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6de82b-0763-44c2-9f1e-95b63190e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = agP()\n",
    " # Set DDP variables (for distributed training)\n",
    "opt.world_size = int(os.environ['WORLD_SIZE']) if 'WORLD_SIZE' in os.environ else 1 # only one node\n",
    "opt.global_rank = int(os.environ['RANK']) if 'RANK' in os.environ else -1 # rank of the gpu in a single node\n",
    "opt.total_batch_size = opt.batch_size\n",
    "\n",
    "# Hyperparameters (using the base file)\n",
    "with open(opt.hyp) as f:\n",
    "    hyp = yaml.load(f, Loader=yaml.SafeLoader) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474b04d-30b6-446f-8a72-7f5f22e00784",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24354819-a338-4b7d-84af-4590c5e70d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hyp, opt, device):\n",
    "\n",
    "    # extracting logs & hyper-param variables....\n",
    "    epochs, batch_size, total_batch_size, weights, rank, freeze = opt.epochs, opt.batch_size, opt.total_batch_size, opt.weights, opt.global_rank, opt.freeze\n",
    "\n",
    "    opt.hyp = hyp\n",
    "    data_dict = None\n",
    "    with open(opt.data) as f:\n",
    "        data_dict = yaml.load(f, Loader=yaml.SafeLoader) \n",
    "\n",
    "    # number of classes\n",
    "    nc = 1 if opt.single_cls else int(data_dict['nc'])  \n",
    "    names = ['item'] if opt.single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names\n",
    "    print(\"Number of classes : \",len(names))\n",
    "    \n",
    "\n",
    "    if opt.open_world:\n",
    "        print(\"Updating number of classes from {} to {} for open world training.\".format(nc, opt.open_world_emb_size))\n",
    "        \n",
    "        # updating nc\n",
    "        nc = opt.open_world_emb_size\n",
    "\n",
    "        # model check (ensuring open world model config file is being called....)\n",
    "        assert \"open_world\" in opt.cfg \n",
    "\n",
    "        # loading class vectors from parent model....\n",
    "        vector_pickle_paths = \"/Users/vedant_j/Desktop/open_world/yolov7_open_world/className_embeddings.pickle\"\n",
    "        with open(vector_pickle_paths, 'rb') as handle:\n",
    "            # the order in which classes.txt is created is the order in which elements of the text pickle are populated\n",
    "            data_dict[\"vectors\"] = pickle.load(handle)\n",
    "\n",
    "            # mapping to be used to get the correct class vector...\n",
    "            data_dict[\"number_to_class\"] = {ctr : cName for ctr, cName in enumerate(data_dict[\"vectors\"].keys())}\n",
    "            class_vector_dict = {ctr : embVal for ctr, embVal in enumerate(data_dict[\"vectors\"].values()) if ctr < data_dict['nc']}\n",
    "        print(\"Loaded vectors count : \",len(class_vector_dict))\n",
    "    \n",
    "   \n",
    "    # Model (PARTIAL WEIGHT LOADING FOR OPEN WORLD MODELS....)\n",
    "    pretrained = weights.endswith('.pt')\n",
    "   \n",
    "    # fresh model instantiation......\n",
    "    model = Model(opt.cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "\n",
    "    # Loading open world.....\n",
    "    if opt.open_world:\n",
    "        print(\"Fresh OPEN WORLD MODEL with base supervised weights....\")\n",
    "        # the open world trianing always starts from supervised pre-trained\n",
    "        ckpt = torch.load(weights, map_location=device)\n",
    "        exclude = ['anchor'] if (opt.cfg or hyp.get('anchors')) and not opt.resume else []  # exclude keys\n",
    "\n",
    "        # LAYER 105 for corresponds to the object detection HEAD that we are trying to modify....\n",
    "        exclude.extend(['model.105.anchors', 'model.105.anchor_grid', 'model.105.m.0.weight', \n",
    "                        'model.105.m.0.bias', 'model.105.m.1.weight', 'model.105.m.1.bias', \n",
    "                        'model.105.m.2.weight', 'model.105.m.2.bias'])\n",
    "        \n",
    "        # loading only the backbone weights + neck weights for supervised YOLO\n",
    "        state_dict = ckpt['model'].float().state_dict()  # to FP32\n",
    "\n",
    "        # Only loading weights for base & neck.....\n",
    "        state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  # intersect\n",
    "        # loading open-world with SUPERVISED YOLO WEIGHTS\n",
    "        model.load_state_dict(state_dict, strict=False) \n",
    "\n",
    "\n",
    "\n",
    "    # dataset path load ....\n",
    "    train_path = data_dict['train']\n",
    "    test_path = data_dict['val']\n",
    "    print(\"Train path : \",train_path)\n",
    "    print(\"Valid path : \",test_path)\n",
    "\n",
    "\n",
    "    # Freeze [custom frozen layers ...]\n",
    "    freeze = [f'model.{x}.' for x in (freeze if len(freeze) > 1 else range(freeze[0]))]  # parameter names to freeze (full or partial)\n",
    "    for k, v in model.named_parameters():\n",
    "        v.requires_grad = True  # train all layers\n",
    "\n",
    "        # freezing the named layers\n",
    "        if any(x in k for x in freeze):\n",
    "            print('freezing %s' % k)\n",
    "            v.requires_grad = False\n",
    "\n",
    "    # Training hyper-params definition \n",
    "    # Optimizer\n",
    "    nbs = 64  # nominal batch size\n",
    "    accumulate = max(round(nbs / total_batch_size), 1)  # accumulate loss before optimizing\n",
    "    hyp['weight_decay'] *= total_batch_size * accumulate / nbs  # scale weight_decay\n",
    "    \n",
    "\n",
    "    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
    "    for k, v in model.named_modules():\n",
    "        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):\n",
    "            pg2.append(v.bias)  # biases\n",
    "        if isinstance(v, nn.BatchNorm2d):\n",
    "            pg0.append(v.weight)  # no decay\n",
    "        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):\n",
    "            pg1.append(v.weight)  # apply decay\n",
    "        if hasattr(v, 'im'):\n",
    "            if hasattr(v.im, 'implicit'):           \n",
    "                pg0.append(v.im.implicit)\n",
    "            else:\n",
    "                for iv in v.im:\n",
    "                    pg0.append(iv.implicit)\n",
    "        if hasattr(v, 'imc'):\n",
    "            if hasattr(v.imc, 'implicit'):           \n",
    "                pg0.append(v.imc.implicit)\n",
    "            else:\n",
    "                for iv in v.imc:\n",
    "                    pg0.append(iv.implicit)\n",
    "        if hasattr(v, 'imb'):\n",
    "            if hasattr(v.imb, 'implicit'):           \n",
    "                pg0.append(v.imb.implicit)\n",
    "            else:\n",
    "                for iv in v.imb:\n",
    "                    pg0.append(iv.implicit)\n",
    "        if hasattr(v, 'imo'):\n",
    "            if hasattr(v.imo, 'implicit'):           \n",
    "                pg0.append(v.imo.implicit)\n",
    "            else:\n",
    "                for iv in v.imo:\n",
    "                    pg0.append(iv.implicit)\n",
    "        if hasattr(v, 'ia'):\n",
    "            if hasattr(v.ia, 'implicit'):           \n",
    "                pg0.append(v.ia.implicit)\n",
    "            else:\n",
    "                for iv in v.ia:\n",
    "                    pg0.append(iv.implicit)\n",
    "        if hasattr(v, 'attn'):\n",
    "            if hasattr(v.attn, 'logit_scale'):   \n",
    "                pg0.append(v.attn.logit_scale)\n",
    "            if hasattr(v.attn, 'q_bias'):   \n",
    "                pg0.append(v.attn.q_bias)\n",
    "            if hasattr(v.attn, 'v_bias'):  \n",
    "                pg0.append(v.attn.v_bias)\n",
    "            if hasattr(v.attn, 'relative_position_bias_table'):  \n",
    "                pg0.append(v.attn.relative_position_bias_table)\n",
    "        if hasattr(v, 'rbr_dense'):\n",
    "            if hasattr(v.rbr_dense, 'weight_rbr_origin'):  \n",
    "                pg0.append(v.rbr_dense.weight_rbr_origin)\n",
    "            if hasattr(v.rbr_dense, 'weight_rbr_avg_conv'): \n",
    "                pg0.append(v.rbr_dense.weight_rbr_avg_conv)\n",
    "            if hasattr(v.rbr_dense, 'weight_rbr_pfir_conv'):  \n",
    "                pg0.append(v.rbr_dense.weight_rbr_pfir_conv)\n",
    "            if hasattr(v.rbr_dense, 'weight_rbr_1x1_kxk_idconv1'): \n",
    "                pg0.append(v.rbr_dense.weight_rbr_1x1_kxk_idconv1)\n",
    "            if hasattr(v.rbr_dense, 'weight_rbr_1x1_kxk_conv2'):   \n",
    "                pg0.append(v.rbr_dense.weight_rbr_1x1_kxk_conv2)\n",
    "            if hasattr(v.rbr_dense, 'weight_rbr_gconv_dw'):   \n",
    "                pg0.append(v.rbr_dense.weight_rbr_gconv_dw)\n",
    "            if hasattr(v.rbr_dense, 'weight_rbr_gconv_pw'):   \n",
    "                pg0.append(v.rbr_dense.weight_rbr_gconv_pw)\n",
    "            if hasattr(v.rbr_dense, 'vector'):   \n",
    "                pg0.append(v.rbr_dense.vector)\n",
    "\n",
    "    if opt.adam:\n",
    "        optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "    else:\n",
    "        optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "\n",
    "    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
    "    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
    "    del pg0, pg1, pg2\n",
    "\n",
    "    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR\n",
    "    if opt.linear_lr:\n",
    "        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear\n",
    "    else:\n",
    "        lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "    \n",
    "\n",
    "    # EMA\n",
    "    ema = ModelEMA(model) if rank in [-1, 0] else None\n",
    "\n",
    "    # Resume\n",
    "    start_epoch, best_fitness = 0, 0.0\n",
    "    \n",
    "    # Image sizes\n",
    "    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "    nl = model.model[-1].nl  # number of detection layers (used for scaling hyp['obj'])\n",
    "    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples\n",
    "\n",
    "    # DP mode\n",
    "    cuda = False\n",
    "    if cuda and rank == -1 and torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # SyncBatchNorm (in distributed training this is a very important flag to set)\n",
    "    if opt.sync_bn and cuda and rank != -1:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\n",
    "\n",
    "\n",
    "    # keeping augmentations off for open world training.... (first phase)\n",
    "    dataloader, dataset = create_dataloader(test_path, \n",
    "                                            imgsz, \n",
    "                                            batch_size, \n",
    "                                            gs, \n",
    "                                            opt,\n",
    "                                            hyp=hyp, \n",
    "                                            augment=False, \n",
    "                                            cache=opt.cache_images, \n",
    "                                            rect=opt.rect, \n",
    "                                            rank=rank,\n",
    "                                            world_size=opt.world_size, \n",
    "                                            workers=opt.workers,\n",
    "                                            image_weights=opt.image_weights, \n",
    "                                            quad=opt.quad, \n",
    "                                            prefix=colorstr('train: '))\n",
    "\n",
    "\n",
    "    \n",
    "    nb = len(dataloader)  # number of batches\n",
    "    print(\"Number of batches :\",nb)\n",
    "\n",
    "    \n",
    "\n",
    "    # DDP mode (incase rank is speicified)\n",
    "    if cuda and rank != -1:\n",
    "        model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank,\n",
    "                    # nn.MultiheadAttention incompatibility with DDP https://github.com/pytorch/pytorch/issues/26698\n",
    "                    find_unused_parameters=any(isinstance(layer, nn.MultiheadAttention) for layer in model.modules()))\n",
    "\n",
    "    # Model parameters\n",
    "\n",
    "    # yolo loss parameters.....\n",
    "    hyp['box'] *= 3. / nl  # scale to layers\n",
    "    hyp['cls'] *= nc / 80. * 3. / nl  # scale to classes and layers\n",
    "    hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  # scale to image size and layers\n",
    "    hyp['label_smoothing'] = opt.label_smoothing\n",
    "    model.nc = nc  # attach number of classes to model\n",
    "    model.hyp = hyp  # attach hyperparameters to model\n",
    "    model.gr = 1.0  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "    if not opt.open_world:\n",
    "        model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights\n",
    "    else:\n",
    "        # for open  world case classes don't exist per se....\n",
    "        model.class_weights = None\n",
    "    \n",
    "    model.names = names\n",
    "\n",
    "\n",
    "    # Start training\n",
    "    t0 = time.time()\n",
    "    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
    "    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training\n",
    "    maps = np.zeros(nc)  # mAP per class\n",
    "    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\n",
    "    scheduler.last_epoch = start_epoch - 1  # do not move\n",
    "    scaler = amp.GradScaler(enabled=cuda)\n",
    "    \n",
    "    # initialzing loss classes....\n",
    "    if not opt.open_world:\n",
    "        compute_loss_ota = ComputeLossOTA(model)  # init loss class\n",
    "        compute_loss = ComputeLoss(model)  # init loss class\n",
    "\n",
    "    else:\n",
    "        compute_loss_ota = ComputeLossOTA_CLIP(model, clip_text_vectors = class_vector_dict)\n",
    "        compute_loss = ComputeLoss_CLIP(model, clip_text_vectors = class_vector_dict)  # init loss class\n",
    "\n",
    "\n",
    "    print(\"Starting to train\")\n",
    "\n",
    "\n",
    "    # Initiating training sequence .....\n",
    "    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
    "        # This operation sets the flag : model.training\n",
    "        model.train()\n",
    "\n",
    "        # Not focusing on this for open world training....\n",
    "        # Update image weights (optional)\n",
    "\n",
    "        mloss = torch.zeros(4, device=device)  # mean losses\n",
    "        if rank != -1:\n",
    "            dataloader.sampler.set_epoch(epoch)\n",
    "            \n",
    "        pbar = enumerate(dataloader)\n",
    "        if rank in [-1, 0]:\n",
    "            pbar = tqdm(pbar, total=nb)  # progress bar\n",
    "\n",
    "        # intializing back prop...\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # loading each batch.....\n",
    "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "            ni = i + nb * epoch  # number integrated batches (since train start) [total count of seen batches]\n",
    "            imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "\n",
    "            # Warmup (warmup to avoid getting stuck in local optima on the loss surface)\n",
    "            if ni <= nw:\n",
    "                xi = [0, nw]  # x interp\n",
    "                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "                accumulate = max(1, np.interp(ni, xi, [1, nbs / total_batch_size]).round())\n",
    "                for j, x in enumerate(optimizer.param_groups):\n",
    "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                    if 'momentum' in x:\n",
    "                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
    "\n",
    "            # Multi-scale (opt is from read cmd line args)\n",
    "            if opt.multi_scale:\n",
    "                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
    "                sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "                if sf != 1:\n",
    "                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
    "                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Forward (amp : mixed precision training)\n",
    "            with amp.autocast(enabled=cuda):\n",
    "                pred = model(imgs)  # forward pass\n",
    "\n",
    "                # loss computation (embeddings will be loaded on GPU)\n",
    "                if 'loss_ota' not in hyp or hyp['loss_ota'] == 1:\n",
    "                    loss, loss_items = compute_loss_ota(pred, \n",
    "                                                        targets.to(device), \n",
    "                                                        imgs)  # loss scaled by batch_size\n",
    "                else:\n",
    "                    loss, loss_items = compute_loss(pred, \n",
    "                                                    targets.to(device))  # loss scaled by batch_size\n",
    "                if rank != -1:\n",
    "                    loss *= opt.world_size  # gradient averaged between devices in DDP mode\n",
    "                if opt.quad:\n",
    "                    loss *= 4.\n",
    "\n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Optimize\n",
    "            if ni % accumulate == 0:\n",
    "                scaler.step(optimizer)  # optimizer.step\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                if ema:\n",
    "                    ema.update(model)\n",
    "\n",
    "            # Print\n",
    "            if rank in [-1, 0]:\n",
    "                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "                mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n",
    "                s = ('%10s' * 2 + '%10.4g' * 6) % (\n",
    "                    '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\n",
    "                pbar.set_description(s)\n",
    "\n",
    "\n",
    "            # end batch ------------------------------------------------------------------------------------------------\n",
    "        # end epoch ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Scheduler\n",
    "        lr = [x['lr'] for x in optimizer.param_groups]  # for tensorboard\n",
    "        scheduler.step()\n",
    "\n",
    "        # DDP process 0 or single-GPU\n",
    "        if rank in [-1, 0]:\n",
    "            # mAP\n",
    "            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride', 'class_weights'])\n",
    "            final_epoch = epoch + 1 == epochs\n",
    "            if not opt.notest or final_epoch:  # Calculate mAP\n",
    "                wandb_logger.current_epoch = epoch + 1\n",
    "                results, maps, times = test.test(data_dict,\n",
    "                                                 batch_size=batch_size * 2,\n",
    "                                                 imgsz=imgsz_test,\n",
    "                                                 model=ema.ema,\n",
    "                                                 single_cls=opt.single_cls,\n",
    "                                                 dataloader=testloader,\n",
    "                                                 save_dir=save_dir,\n",
    "                                                 verbose=nc < 50 and final_epoch,\n",
    "                                                 plots=plots and final_epoch,\n",
    "                                                 wandb_logger=wandb_logger,\n",
    "                                                 compute_loss=compute_loss,\n",
    "                                                 is_coco=is_coco,\n",
    "                                                 v5_metric=opt.v5_metric)\n",
    "\n",
    "            # Write\n",
    "            with open(results_file, 'a') as f:\n",
    "                f.write(s + '%10.4g' * 7 % results + '\\n')  # append metrics, val_loss\n",
    "            if len(opt.name) and opt.bucket:\n",
    "                os.system('gsutil cp %s gs://%s/results/results%s.txt' % (results_file, opt.bucket, opt.name))\n",
    "\n",
    "            # Log\n",
    "            tags = ['train/box_loss', 'train/obj_loss', 'train/cls_loss',  # train loss\n",
    "                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',\n",
    "                    'val/box_loss', 'val/obj_loss', 'val/cls_loss',  # val loss\n",
    "                    'x/lr0', 'x/lr1', 'x/lr2']  # params\n",
    "            for x, tag in zip(list(mloss[:-1]) + list(results) + lr, tags):\n",
    "                if tb_writer:\n",
    "                    tb_writer.add_scalar(tag, x, epoch)  # tensorboard\n",
    "                if wandb_logger.wandb:\n",
    "                    wandb_logger.log({tag: x})  # W&B\n",
    "\n",
    "            # Update best mAP\n",
    "            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]\n",
    "            if fi > best_fitness:\n",
    "                best_fitness = fi\n",
    "            wandb_logger.end_epoch(best_result=best_fitness == fi)\n",
    "\n",
    "            # Save model\n",
    "            if (not opt.nosave) or (final_epoch and not opt.evolve):  # if save\n",
    "                ckpt = {'epoch': epoch,\n",
    "                        'best_fitness': best_fitness,\n",
    "                        'training_results': results_file.read_text(),\n",
    "                        'model': deepcopy(model.module if is_parallel(model) else model).half(),\n",
    "                        'ema': deepcopy(ema.ema).half(),\n",
    "                        'updates': ema.updates,\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'wandb_id': wandb_logger.wandb_run.id if wandb_logger.wandb else None}\n",
    "\n",
    "                # Save last, best and delete\n",
    "                torch.save(ckpt, last)\n",
    "                if best_fitness == fi:\n",
    "                    torch.save(ckpt, best)\n",
    "                if (best_fitness == fi) and (epoch >= 200):\n",
    "                    torch.save(ckpt, wdir / 'best_{:03d}.pt'.format(epoch))\n",
    "                if epoch == 0:\n",
    "                    torch.save(ckpt, wdir / 'epoch_{:03d}.pt'.format(epoch))\n",
    "                elif ((epoch+1) % 25) == 0:\n",
    "                    torch.save(ckpt, wdir / 'epoch_{:03d}.pt'.format(epoch))\n",
    "                elif epoch >= (epochs-5):\n",
    "                    torch.save(ckpt, wdir / 'epoch_{:03d}.pt'.format(epoch))\n",
    "                if wandb_logger.wandb:\n",
    "                    if ((epoch + 1) % opt.save_period == 0 and not final_epoch) and opt.save_period != -1:\n",
    "                        wandb_logger.log_model(\n",
    "                            last.parent, opt, epoch, fi, best_model=best_fitness == fi)\n",
    "                del ckpt\n",
    "\n",
    "        # end epoch ----------------------------------------------------------------------------------------------------\n",
    "    # end training\n",
    "    if rank in [-1, 0]:\n",
    "        # Plots\n",
    "        if plots:\n",
    "            plot_results(save_dir=save_dir)  # save as results.png\n",
    "            if wandb_logger.wandb:\n",
    "                files = ['results.png', 'confusion_matrix.png', *[f'{x}_curve.png' for x in ('F1', 'PR', 'P', 'R')]]\n",
    "                wandb_logger.log({\"Results\": [wandb_logger.wandb.Image(str(save_dir / f), caption=f) for f in files\n",
    "                                              if (save_dir / f).exists()]})\n",
    "        # Test best.pt\n",
    "        logger.info('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\n",
    "        if opt.data.endswith('coco.yaml') and nc == 80:  # if COCO\n",
    "            for m in (last, best) if best.exists() else (last):  # speed, mAP tests\n",
    "                results, _, _ = test.test(opt.data,\n",
    "                                          batch_size=batch_size * 2,\n",
    "                                          imgsz=imgsz_test,\n",
    "                                          conf_thres=0.001,\n",
    "                                          iou_thres=0.7,\n",
    "                                          model=attempt_load(m, device).half(),\n",
    "                                          single_cls=opt.single_cls,\n",
    "                                          dataloader=testloader,\n",
    "                                          save_dir=save_dir,\n",
    "                                          save_json=True,\n",
    "                                          plots=False,\n",
    "                                          is_coco=is_coco,\n",
    "                                          v5_metric=opt.v5_metric)\n",
    "\n",
    "        # Strip optimizers\n",
    "        final = best if best.exists() else last  # final model\n",
    "        for f in last, best:\n",
    "            if f.exists():\n",
    "                strip_optimizer(f)  # strip optimizers\n",
    "        if opt.bucket:\n",
    "            os.system(f'gsutil cp {final} gs://{opt.bucket}/weights')  # upload\n",
    "        if wandb_logger.wandb and not opt.evolve:  # Log the stripped model\n",
    "            wandb_logger.wandb.log_artifact(str(final), type='model',\n",
    "                                            name='run_' + wandb_logger.wandb_run.id + '_model',\n",
    "                                            aliases=['last', 'best', 'stripped'])\n",
    "        wandb_logger.finish_run()\n",
    "    else:\n",
    "        dist.destroy_process_group()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f531e57-0f1d-4b19-a810-87964a328ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes :  65\n",
      "Updating number of classes from 65 to 512 for open world training.\n",
      "Loaded vectors count :  65\n",
      "Fresh OPEN WORLD MODEL with base supervised weights....\n",
      "Train path :  /Users/vedant_j/Desktop/open_world/yolo_coco_dataset_65_15/train/images\n",
      "Valid path :  /Users/vedant_j/Desktop/open_world/yolo_coco_dataset_65_15/val_yolo/images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/Users/vedant_j/Desktop/open_world/yolo_coco_dataset_65_15/val_yolo/label_clip_vecto\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /Users/vedant_j/Desktop/open_world/yolo_coco_dataset_65_15/val_yolo/images/COCO_train2014_000000214087.jpg: duplicate labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/Users/vedant_j/Desktop/open_world/yolo_coco_dataset_65_15/val_yolo/label_clip_vecto\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16343\n",
      "Number of batches : 8172\n",
      "Loaded CLIP text classification vectors :  torch.Size([65, 512])\n",
      "Starting to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      0/49        0G   0.07504  0.002552     478.1     478.2        26       224:   1%| | 93/8172 [00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 310\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(hyp, opt, device)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Forward (amp : mixed precision training)\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mcuda):\n\u001b[0;32m--> 310\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# loss computation (embeddings will be loaded on GPU)\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_ota\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m hyp \u001b[38;5;129;01mor\u001b[39;00m hyp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_ota\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/clip_exp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/open_world/yolov7_open_world/models/yolo_zsd.py:789\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x, augment, profile)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# augmented inference, train\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/open_world/yolov7_open_world/models/yolo_zsd.py:815\u001b[0m, in \u001b[0;36mModel.forward_once\u001b[0;34m(self, x, profile)\u001b[0m\n\u001b[1;32m    812\u001b[0m         dt\u001b[38;5;241m.\u001b[39mappend((time_synchronized() \u001b[38;5;241m-\u001b[39m t) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%10.1f\u001b[39;00m\u001b[38;5;132;01m%10.0f\u001b[39;00m\u001b[38;5;132;01m%10.1f\u001b[39;00m\u001b[38;5;124mms \u001b[39m\u001b[38;5;132;01m%-40s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (o, m\u001b[38;5;241m.\u001b[39mnp, dt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], m\u001b[38;5;241m.\u001b[39mtype))\n\u001b[0;32m--> 815\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    817\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/clip_exp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/open_world/yolov7_open_world/models/common.py:108\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/clip_exp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/clip_exp/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/clip_exp/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(hyp, opt, device = torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961211f0-e618-4e92-be64-9fcded32e604",
   "metadata": {},
   "source": [
    "## Test BED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c005a10-e7f1-4dcb-a605-34dea4b726ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for coup in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b951e9a-38cd-49ad-a6cd-04faa6985b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, labels_out, paths, shapes = coup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92d3bb1-bb2c-4eb6-9f95-5318abd68954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102efddc-6ea9-465e-b8b6-7adeac15f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = []\n",
    "ymin = []\n",
    "w_list = []\n",
    "h_list = []\n",
    "labs_list = []\n",
    "cnt = 10\n",
    "im = Image.fromarray(img[cnt].permute(1, 2, 0).numpy())\n",
    "\n",
    "\n",
    "for i in labels_out:\n",
    "    if i[0] == cnt:\n",
    "        x_c = (i[2]*640).numpy()\n",
    "        y_c = (i[3]*640).numpy()\n",
    "        w = (i[4]*640).numpy()\n",
    "        h = (i[5]*640).numpy()\n",
    "        xmin.append(x_c - w/2)\n",
    "        ymin.append(y_c - h/2)\n",
    "        w_list.append(w)\n",
    "        h_list.append(h)\n",
    "        labs_list.append(i[1])\n",
    "\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im)\n",
    "\n",
    "for x_i,y_i,w_i,h_i in zip(xmin,ymin,w_list,h_list):\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((x_i, y_i), w_i, h_i, linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b59fc9-fd11-401b-ac9b-3f0db9411ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18591298-3b33-4489-82c6-d41ee959d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_pickle_paths = \"/Users/vedant_j/Desktop/open_world/yolov7_open_world/className_embeddings.pickle\"\n",
    "data_dict = {'nc' : 65}\n",
    "with open(vector_pickle_paths, 'rb') as handle:\n",
    "    # the order in which classes.txt is created is the order in which elements of the text pickle are populated\n",
    "    data_dict[\"vectors\"] = pickle.load(handle)\n",
    "\n",
    "    # mapping to be used to get the correct class vector...\n",
    "    data_dict[\"number_to_class\"] = {ctr : cName for ctr, cName in enumerate(data_dict[\"vectors\"].keys())}\n",
    "    class_vector_dict = {ctr : embVal for ctr, embVal in enumerate(data_dict[\"vectors\"].values()) if ctr < data_dict['nc']}\n",
    "print(\"Loaded vectors count : \",len(class_vector_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ce9dd-3493-4869-a9ca-c78d7c633ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = torch.ones(7, device=torch.device('cpu')).long() \n",
    "targets = torch.from_numpy(np.array([[[1.0,2.0,0.4,0.6,0.7,0.8],\n",
    "                                     [2.0,2.0,0.3,0.6,0.4,0.8]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741866f6-2b83-4d2d-82a2-967993f39c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets*gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a24b09-12bc-433c-9364-ae3aab365d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.from_numpy(np.array([[1,3,4,5,6],\n",
    "                                [4,3,4,5,2],\n",
    "                                [5,3,4,5,4],\n",
    "                                [6,7,7,7,6]], dtype=np.float32))\n",
    "\n",
    "\n",
    "t2 = torch.from_numpy(np.array([[2,8,10,11,1],\n",
    "                                [4,3,4,5,7],\n",
    "                                [6,7,4,1,2]], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554ad23-d6bc-4c86-b0d5-71c1605fb4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_vec  = torch.zeros((t1.shape[0], t2.shape[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b5e5d-4fc8-4824-a086-dd3cb29ce551",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_norm = t1 / t1.norm(dim=1)[:, None]\n",
    "b_norm = t2 / t2.norm(dim=1)[:, None]\n",
    "res = torch.mm(a_norm, b_norm.transpose(0,1))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641175f-b80b-429c-a52d-0a21891bb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee90da-d1b9-4548-942e-69c15b4213fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.from_numpy(np.array([0,2,2,1]))\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb209d-e66e-4473-a808-fda020b15055",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.NLLLoss(reduction=\"mean\")(res.log(), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50823d8-a5e9-4d27-ad7e-de67fb32c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f9854-249d-469b-9a86-007d545bd929",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(0.95976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bbda250-dd9c-4af1-b083-d77dbae556e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"/train/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53180a23-b52d-48d6-ae74-6193a34321b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/train/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa579a-9da2-4c0d-9dd2-c39b68f862d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
